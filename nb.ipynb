{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  article dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as Datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm as tqdm_regular\n",
    "import seaborn as sns\n",
    "from torchvision.utils import make_grid\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Running on the GPU')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Running on the CPU')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_images(dataset):\n",
    "  \"\"\"\n",
    "  This function extracts images of cats (index 3)\n",
    "  and dogs (index 5) from the CIFAR-10 dataset.\n",
    "  \"\"\"\n",
    "  cats = []\n",
    "  dogs = []\n",
    "\n",
    "  for idx in tqdm_regular(range(len(dataset))):\n",
    "    if dataset.targets[idx]==3:\n",
    "      cats.append((dataset.data[idx], 0))\n",
    "    elif dataset.targets[idx]==5:\n",
    "      dogs.append((dataset.data[idx], 1))\n",
    "    else:\n",
    "      pass\n",
    "  return cats, dogs\n",
    "\n",
    "\n",
    "#  loading training data\n",
    "training_set = Datasets.CIFAR10(root='./', download=True,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "#  loading validation data\n",
    "validation_set = Datasets.CIFAR10(root='./', download=True, train=False,\n",
    "                                transform=transforms.ToTensor())\n",
    "                                \n",
    "                                  \n",
    "#  extracting training images\n",
    "cat_train, dog_train = extract_images(training_set)\n",
    "training_data = cat_train + dog_train\n",
    "random.shuffle(training_data)\n",
    "\n",
    "#  extracting validation images\n",
    "cat_val, dog_val = extract_images(validation_set)\n",
    "validation_data = cat_val + dog_val\n",
    "random.shuffle(training_data)\n",
    "\n",
    "\n",
    "#  removing labels\n",
    "training_images = [x[0] for x in training_data]\n",
    "validation_images = [x[0] for x in validation_data]\n",
    "test_images = [x[0] for x in cat_val[:5]] + [x[0] for x in dog_val[:5]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  defining dataset class\n",
    "class CustomCIFAR10(Dataset):\n",
    "  def __init__(self, data, transforms=None):\n",
    "    self.data = data\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image = self.data[idx]\n",
    "\n",
    "    if self.transforms!=None:\n",
    "      image = self.transforms(image)\n",
    "    return image\n",
    "    \n",
    "    \n",
    "#  creating pytorch datasets\n",
    "training_data = CustomCIFAR10(training_images, transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "validation_data = CustomCIFAR10(validation_images, transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "test_data = CustomCIFAR10(test_images, transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  defining encoder\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, in_channels=3, out_channels=16, latent_dim=1000, act_fn=nn.ReLU()):\n",
    "    super().__init__()\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1), # (32, 32)\n",
    "        act_fn,\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1), \n",
    "        act_fn,\n",
    "        nn.Conv2d(out_channels, 2*out_channels, 3, padding=1, stride=2), # (16, 16)\n",
    "        act_fn,\n",
    "        nn.Conv2d(2*out_channels, 2*out_channels, 3, padding=1),\n",
    "        act_fn,\n",
    "        nn.Conv2d(2*out_channels, 4*out_channels, 3, padding=1, stride=2), # (8, 8)\n",
    "        act_fn,\n",
    "        nn.Conv2d(4*out_channels, 4*out_channels, 3, padding=1),\n",
    "        act_fn,\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(4*out_channels*8*8, latent_dim),\n",
    "        act_fn\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(-1, 3, 32, 32)\n",
    "    output = self.net(x)\n",
    "    return output\n",
    "\n",
    "\n",
    "#  defining decoder\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, in_channels=3, out_channels=16, latent_dim=1000, act_fn=nn.ReLU()):\n",
    "    super().__init__()\n",
    "\n",
    "    self.out_channels = out_channels\n",
    "\n",
    "    self.linear = nn.Sequential(\n",
    "        nn.Linear(latent_dim, 4*out_channels*8*8),\n",
    "        act_fn\n",
    "    )\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.ConvTranspose2d(4*out_channels, 4*out_channels, 3, padding=1), # (8, 8)\n",
    "        act_fn,\n",
    "        nn.ConvTranspose2d(4*out_channels, 2*out_channels, 3, padding=1, \n",
    "                           stride=2, output_padding=1), # (16, 16)\n",
    "        act_fn,\n",
    "        nn.ConvTranspose2d(2*out_channels, 2*out_channels, 3, padding=1),\n",
    "        act_fn,\n",
    "        nn.ConvTranspose2d(2*out_channels, out_channels, 3, padding=1, \n",
    "                           stride=2, output_padding=1), # (32, 32)\n",
    "        act_fn,\n",
    "        nn.ConvTranspose2d(out_channels, out_channels, 3, padding=1),\n",
    "        act_fn,\n",
    "        nn.ConvTranspose2d(out_channels, in_channels, 3, padding=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.linear(x)\n",
    "    output = output.view(-1, 4*self.out_channels, 8, 8)\n",
    "    output = self.conv(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "#  defining autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.encoder.to(device)\n",
    "\n",
    "    self.decoder = decoder\n",
    "    self.decoder.to(device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ConvolutionalAutoencoder():\n",
    "  def __init__(self, autoencoder):\n",
    "    self.network = autoencoder\n",
    "    self.optimizer = torch.optim.Adam(self.network.parameters(), lr=1e-3)\n",
    "\n",
    "  def train(self, loss_function, epochs, batch_size, \n",
    "            training_set, validation_set, test_set):\n",
    "    \n",
    "    #  creating log\n",
    "    log_dict = {\n",
    "        'training_loss_per_batch': [],\n",
    "        'validation_loss_per_batch': [],\n",
    "        'visualizations': []\n",
    "    } \n",
    "\n",
    "    #  defining weight initialization function\n",
    "    def init_weights(module):\n",
    "      if isinstance(module, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "      elif isinstance(module, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "\n",
    "    #  initializing network weights\n",
    "    self.network.apply(init_weights)\n",
    "\n",
    "    #  creating dataloaders\n",
    "    train_loader = DataLoader(training_set, batch_size)\n",
    "    val_loader = DataLoader(validation_set, batch_size)\n",
    "    test_loader = DataLoader(test_set, 10)\n",
    "\n",
    "    #  setting convnet to training mode\n",
    "    self.network.train()\n",
    "    self.network.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      print(f'Epoch {epoch+1}/{epochs}')\n",
    "      train_losses = []\n",
    "\n",
    "      #------------\n",
    "      #  TRAINING\n",
    "      #------------\n",
    "      print('training...')\n",
    "      for images in tqdm(train_loader):\n",
    "        #  zeroing gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        #  sending images to device\n",
    "        images = images.to(device)\n",
    "        #  reconstructing images\n",
    "        output = self.network(images)\n",
    "        #  computing loss\n",
    "        loss = loss_function(output, images.view(-1, 3, 32, 32))\n",
    "        #  calculating gradients\n",
    "        loss.backward()\n",
    "        #  optimizing weights\n",
    "        self.optimizer.step()\n",
    "\n",
    "        #--------------\n",
    "        # LOGGING\n",
    "        #--------------\n",
    "        log_dict['training_loss_per_batch'].append(loss.item())\n",
    "\n",
    "      #--------------\n",
    "      # VALIDATION\n",
    "      #--------------\n",
    "      print('validating...')\n",
    "      for val_images in tqdm(val_loader):\n",
    "        with torch.no_grad():\n",
    "          #  sending validation images to device\n",
    "          val_images = val_images.to(device)\n",
    "          #  reconstructing images\n",
    "          output = self.network(val_images)\n",
    "          #  computing validation loss\n",
    "          val_loss = loss_function(output, val_images.view(-1, 3, 32, 32))\n",
    "\n",
    "        #--------------\n",
    "        # LOGGING\n",
    "        #--------------\n",
    "        log_dict['validation_loss_per_batch'].append(val_loss.item())\n",
    "\n",
    "\n",
    "      #--------------\n",
    "      # VISUALISATION\n",
    "      #--------------\n",
    "      print(f'training_loss: {round(loss.item(), 4)} validation_loss: {round(val_loss.item(), 4)}')\n",
    "\n",
    "      for test_images in test_loader:\n",
    "        #  sending test images to device\n",
    "        test_images = test_images.to(device)\n",
    "        with torch.no_grad():\n",
    "          #  reconstructing test images\n",
    "          reconstructed_imgs = self.network(test_images)\n",
    "        #  sending reconstructed and images to cpu to allow for visualization\n",
    "        reconstructed_imgs = reconstructed_imgs.cpu()\n",
    "        test_images = test_images.cpu()\n",
    "\n",
    "        #  visualisation\n",
    "        imgs = torch.stack([test_images.view(-1, 3, 32, 32), reconstructed_imgs], \n",
    "                          dim=1).flatten(0,1)\n",
    "        grid = make_grid(imgs, nrow=10, normalize=True, padding=1)\n",
    "        grid = grid.permute(1, 2, 0)\n",
    "        plt.figure(dpi=170)\n",
    "        plt.title('Original/Reconstructed')\n",
    "        plt.imshow(grid)\n",
    "        log_dict['visualizations'].append(grid)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "      \n",
    "    return log_dict\n",
    "\n",
    "  def autoencode(self, x):\n",
    "    return self.network(x)\n",
    "\n",
    "  def encode(self, x):\n",
    "    encoder = self.network.encoder\n",
    "    return encoder(x)\n",
    "  \n",
    "  def decode(self, x):\n",
    "    decoder = self.network.decoder\n",
    "    return decoder(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  training model\n",
    "model = ConvolutionalAutoencoder(Autoencoder(Encoder(), Decoder()))\n",
    "\n",
    "log_dict = model.train(nn.MSELoss(), epochs=30, batch_size=64, \n",
    "                       training_set=training_data, validation_set=validation_data,\n",
    "                       test_set=test_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def reconstruction_loss(image, model, visualize=True):\n",
    "  \"\"\"\n",
    "  This function calculates the reconstruction loss of an\n",
    "  image for anomaly detection\n",
    "  \"\"\"\n",
    "  #  reading image\n",
    "  image = cv2.imread(image)\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "  image = cv2.resize(image, (32, 32))\n",
    "\n",
    "  #  defining transforms\n",
    "  transform =transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "  image = transform(image)\n",
    "  image = image.view(-1, 3, 32, 32)\n",
    "  image = image.to(device)\n",
    "\n",
    "  #  passing image through autoencoder\n",
    "  with torch.no_grad():\n",
    "    reconstruction = model.autoencode(image)\n",
    "    #  computing reconstruction loss\n",
    "    reconstruction_loss = F.mse_loss(image, reconstruction)\n",
    "\n",
    "  print(f'reconstruction_loss: {round(reconstruction_loss.item(), 4)}')\n",
    "\n",
    "  if visualize:  \n",
    "    #  visualization\n",
    "    grid = make_grid([image.view(3, 32, 32).cpu(), reconstruction.view(3, 32, 32).cpu()], normalize=True, padding=1)\n",
    "    grid = grid.permute(1,2,0)\n",
    "    plt.figure(dpi=100)\n",
    "    plt.title('uploaded/reconstruction')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(grid)\n",
    "  else:\n",
    "    pass\n",
    "\n",
    "  return round(reconstruction_loss.item(), 4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  computing loss\n",
    "recon_loss = reconstruction_loss('cat_1.jpg', model=model)\n",
    "\n",
    "# >>> reconstruction_loss: 0.0447"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  computing loss\n",
    "recon_loss = reconstruction_loss('cat_2.jpg', model=model)\n",
    "\n",
    "# >>> reconstruction_loss: 0.0191"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  computing loss\n",
    "recon_loss = reconstruction_loss('dog_1.jpg', model=model)\n",
    "\n",
    "# >>> reconstruction_loss: 0.0354"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  computing loss\n",
    "recon_loss = reconstruction_loss('dog_2.jpg', model=model)\n",
    "\n",
    "# >>> reconstruction_loss: 0.0399"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  computing loss\n",
    "recon_loss = reconstruction_loss('out_of_sample_1.jpg', model=model)\n",
    "\n",
    "# >>> reconstruction_loss: 0.0726"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  computing loss\n",
    "recon_loss = reconstruction_loss('out_of_sample_2.jpg', model=model)\n",
    "\n",
    "# >>> reconstruction_loss: 0.0581"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  computing loss\n",
    "recon_loss = reconstruction_loss('out_of_sample_3.jpg', model=model)\n",
    "\n",
    "# >>> reconstruction_loss: 0.037"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def aleatoric_screen(image, model):\n",
    "  \"\"\"\n",
    "  This function calculates the reconstruction loss of an\n",
    "  image and acts as a screen against out-of-sample images\n",
    "  \"\"\"\n",
    "  #  reading image\n",
    "  image = cv2.imread(image)\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "  image = cv2.resize(image, (32, 32))\n",
    "\n",
    "  #  defining transforms\n",
    "  transform =transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "  image = transform(image)\n",
    "  image = image.view(-1, 3, 32, 32)\n",
    "  image = image.to(device)\n",
    "\n",
    "  #  passing image through autoencoder\n",
    "  with torch.no_grad():\n",
    "    reconstruction = model.autoencode(image)\n",
    "    #  computing reconstruction loss\n",
    "    reconstruction_loss = F.mse_loss(image, reconstruction)\n",
    "    reconstruction_loss = round(reconstruction_loss.item(), 3)\n",
    "\n",
    "  if reconstruction_loss > 0.045:\n",
    "    print('The model is not built to classify this sort of image')\n",
    "  else:\n",
    "    return image"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}